{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa65a134",
   "metadata": {},
   "source": [
    "# üßº Data Cleaning Notebook ‚Äì Milestone 2\n",
    "\n",
    "**Project:** Gold Pathfinder ML Project  \n",
    "**Notebook:** `01_data_cleaning.ipynb`  \n",
    "\n",
    "This notebook documents how the **raw ALS assay files** are loaded, inspected, and cleaned\n",
    "before being saved into the **`1_datasets/cleaned/`** folder.\n",
    "\n",
    "It mirrors the logic implemented in the Python script:\n",
    "\n",
    "```text\n",
    "2_data_preparation/scripts/data_preparation.py\n",
    "```\n",
    "\n",
    "but presents it in a step-by-step, human-readable form for ELO2 evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d7235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "# Assume this notebook lives in 2_data_preparation/\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "RAW_DIR = PROJECT_ROOT / '1_datasets' / 'raw'\n",
    "CLEANED_DIR = PROJECT_ROOT / '1_datasets' / 'cleaned'\n",
    "\n",
    "CLEANED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROJECT_ROOT, RAW_DIR, CLEANED_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cee3e25",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Inspect Available Raw Files\n",
    "\n",
    "We first inspect which raw CSV files are present in `1_datasets/raw/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2390b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(RAW_DIR.glob('*.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb413795",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Helper Functions for Cleaning\n",
    "\n",
    "We define helper functions to standardize column names and parse values\n",
    "reported below detection limits, like `\"<0.01\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472b7624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_column_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.replace(' ', '_', regex=False)\n",
    "        .str.lower()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_detection_limit(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan, False\n",
    "    if isinstance(value, (int, float)):\n",
    "        return float(value), False\n",
    "    s = str(value).strip()\n",
    "    if not s:\n",
    "        return np.nan, False\n",
    "    if s.startswith('<'):\n",
    "        try:\n",
    "            lod = float(s[1:])\n",
    "            return lod, True\n",
    "        except ValueError:\n",
    "            return np.nan, True\n",
    "    try:\n",
    "        return float(s), False\n",
    "    except ValueError:\n",
    "        return np.nan, False\n",
    "\n",
    "\n",
    "def convert_numeric_with_flag(df: pd.DataFrame, cols):\n",
    "    df = df.copy()\n",
    "    below_detection = pd.Series(False, index=df.index)\n",
    "    for col in cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        parsed_vals = []\n",
    "        flags = []\n",
    "        for v in df[col]:\n",
    "            val, flag = parse_detection_limit(v)\n",
    "            parsed_vals.append(val)\n",
    "            flags.append(flag)\n",
    "        df[col] = parsed_vals\n",
    "        below_detection = below_detection | pd.Series(flags, index=df.index)\n",
    "    return df, below_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0574fcd8",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Example: Clean One Raw File (Core Assays)\n",
    "\n",
    "As an example, we demonstrate how to clean a core assay file\n",
    "(e.g., `An1_Core.csv`). Adjust the filename if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c320b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_file = RAW_DIR / 'An1_Core.csv'  # adjust if different\n",
    "example_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b392517",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_df_raw = pd.read_csv(example_file)\n",
    "core_df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c3c46",
   "metadata": {},
   "source": [
    "### 3.1 Standardize Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aeba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_df = standardize_column_names(core_df_raw)\n",
    "core_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8943470",
   "metadata": {},
   "source": [
    "### 3.2 Select and Rename Key Columns\n",
    "\n",
    "Adjust the mappings below to match your cleaned schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c595619",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_map = {\n",
    "    'field_id': 'sample_id',\n",
    "    'sample_id': 'sample_id',\n",
    "    'lab_id': 'lab_id',\n",
    "    'x': 'easting',\n",
    "    'y': 'northing',\n",
    "    'elevation_from_m': 'elevation_from',\n",
    "    'elevation_to_m': 'elevation_to',\n",
    "    'au_ppm': 'au_ppm',\n",
    "    'au': 'au_ppm',\n",
    "    'as_ppm': 'as_ppm',\n",
    "    'sb_ppm': 'sb_ppm',\n",
    "    'bi_ppm': 'bi_ppm',\n",
    "    'cu_ppm': 'cu_ppm',\n",
    "    'zn_ppm': 'zn_ppm',\n",
    "    'pb_ppm': 'pb_ppm',\n",
    "    'ag_ppm': 'ag_ppm',\n",
    "}\n",
    "\n",
    "clean_core = pd.DataFrame(index=core_df.index)\n",
    "for raw_col, std_col in col_map.items():\n",
    "    if raw_col in core_df.columns:\n",
    "        clean_core[std_col] = core_df[raw_col]\n",
    "\n",
    "clean_core['sample_type'] = 'core'\n",
    "clean_core['project_area'] = 'Shamkya'\n",
    "clean_core['anomaly_id'] = 'An1'\n",
    "\n",
    "clean_core.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a21fcca",
   "metadata": {},
   "source": [
    "### 3.3 Convert Geochemical Columns to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355fb5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\n",
    "    'au_ppm', 'as_ppm', 'sb_ppm', 'bi_ppm',\n",
    "    'cu_ppm', 'zn_ppm', 'pb_ppm', 'ag_ppm',\n",
    "]\n",
    "\n",
    "clean_core, bdl_flag = convert_numeric_with_flag(clean_core, numeric_cols)\n",
    "clean_core['below_detection'] = bdl_flag\n",
    "clean_core.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a6def",
   "metadata": {},
   "source": [
    "### 3.4 Save Cleaned Core Dataset\n",
    "\n",
    "We now save the cleaned core data to:\n",
    "\n",
    "```text\n",
    "1_datasets/cleaned/core_assays_clean.csv\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6ba127",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_out = CLEANED_DIR / 'core_assays_clean.csv'\n",
    "clean_core.to_csv(core_out, index=False)\n",
    "core_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd29d86",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Generalizing to Other Files\n",
    "\n",
    "Repeat similar steps for:\n",
    "\n",
    "- `An1_RC.csv`\n",
    "- `An6_Chip.csv`\n",
    "- `An7_Chip.csv`\n",
    "- `An6-Trenchs_Result.csv`\n",
    "- `An6-Grap.csv`\n",
    "- `An7_Grap.csv`\n",
    "\n",
    "In practice, we use the **Python script** in `2_data_preparation/scripts/`\n",
    "to automate these steps. This notebook serves as documentation and\n",
    "an educational walkthrough for Milestone 2.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
